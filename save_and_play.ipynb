{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d2ee5-4cc3-4cb3-8536-038c5a9c088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='KoboldAI/OPT-13B-Erebus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be370494-f34f-44cd-82aa-32c0ff027218",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Welcome Captain Janeway, I apologize for the delay.\", do_sample=True, min_length=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418c80f-698c-4db1-bbc6-22ba7eca01d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10eca5d-dae9-4dad-8c8e-063bb1d9f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b87747-b3e8-4751-ba74-049f53a6812c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abcb38-c1f6-41b3-829a-ed266f8a7275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab07a8cb470c444e996556983f1eb968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'cache/'\n",
    "import transformers\n",
    "model_name = 'Intel/neural-chat-7b-v3-1'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name,low_cpu_mem_usage=True,cache_dir=\"cache\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,low_cpu_mem_usage=True,cache_dir=\"cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a0f36-c843-4ef4-822a-9b23d9e14771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_input, user_input):\n",
    "\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    return response.split(\"### Assistant:\\n\")[-1]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "system_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\n",
    "user_input = \"calculate 100 + 520 + 60\"\n",
    "response = generate_response(system_input, user_input)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llm",
   "language": "python",
   "name": "venv_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
